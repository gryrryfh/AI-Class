## 1장 딥러닝 시작을 위한 준비 운동
인공지능 안에 머신러닝 딥러닝
딥러닝 실행을 위해 필요한 3가지 : 데이터, 컴퓨터, 프로그램
프로그램 : 구글코랩(설치 필요없음, 구글이 다 제공, 세션유지 힘듦), 주피터 노트북(세선유지 제약 없음, 아나콘다 설치) 등
파이썬 예시(print("Hello, Deeplearning!"))
탠서플로 설치방법 : import tensorflwo as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
import numpy as np
from 라이브러리 import 함수명
데이터 준비하고 로드하고??
텐서플로 : 구글에서 만든 딥러닝 전용 라이브러리
사용법이 어렵지만 케라스로 해결
딥러닝은 여러 층이 쌓여 만들어진다는 것
keras사용예시 : sequential(함수를 모델로 선언), dense(빽빽한 정도)

``` python
from tensorflow.keras.models import Sequential  # 텐서플로의 케라스 API에서 필요한 함수들을 불러옵니다.
from tensorflow.keras.layers import Dense       # 데이터를 다루는데 필요한 라이브러리를 불러옵니다.
import numpy as np
Data_set = np.loadtxt("./data/ThoraricSurgery3.csv", delimiter=",") # 준비된 수술 환자 데이터를 불러옵니다.
X = Data_set[:,0:16]                                                 # 환자의 진찰 기록을 X로 지정합니다.
y = Data_set[:,16]
model = Sequential()                                                  # 딥러닝 모델의 구조를 결정합니다.
model.add(Dense(30, input_dim=16, activation='relu'))
model.add(Dense(1, activation='sigmoid'))
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])  # 딥러닝 모델을 실행합니다.
history=model.fit(X, y, epochs=5, batch_size=16)
```

## 2장 딥러닝을 위한 기초 수학
기울기=a, y절편=b y=ax+b   a=y증가량/x증가량
y=a(x-p)^2+q 최솟값 == q
미분 : 순간변화율을 구하는 것   미분계수==기울기
원하는 한 가지 변수만 미분하고 그 외에는 모두 상수로 취급하는 것이 편미분
시그모이드 함수

딥러닝의 내부를 보면 입력받은 신호를 얼마나 출력할지를 계산하는 과정이 무수히 반복
이때 출력 값으로 얼마나 내보낼지를 계산하는 함수를 활성화 함수라고 함
활성화 함수는 딥러닝이 발전함에 따라 여러 가지 형태로 개발되어 왔는데, 그중 가장 먼저 배우는 중요한 함수가 바로 시그모이드 함수
![image](https://github.com/gryrryfh/AI-Class/assets/50912987/8ee038e6-e679-4913-8f0f-2f0e8d26938c)

## 3장
딥러닝은 자그마한 통계의 결과들이 무수히 얽히고설켜 이루어지는 복잡한 연산의 결정체
선형 회귀(linerar regression), 로지스틱 회귀
독립변수, 종속변수
x값만으로도 y값을 설명할 수 있으면 단순 선형회귀
x값이 여러개 필요하다면 다중선형회귀
최소제곱법![image](https://github.com/gryrryfh/AI-Class/assets/50912987/6f900552-4e2a-4e79-a364-8369ae89014e)
x의 편차(각 값과 평균과의 차이)를 제곱해서 합한 값을
분모로 놓고, x와 y의 편차를 곱해서 합한 값을 분자로 놓으면 기울기
![image](https://github.com/gryrryfh/AI-Class/assets/50912987/6993ec34-6de4-4aea-b44b-9d510f021165)

오차=실제값-예측값
평균제곱오차= (오차)^2의 합/n
![image](https://github.com/gryrryfh/AI-Class/assets/50912987/4f7a77fa-3fc5-4ce8-92c8-c91f039f5997)

### 최소제곱 파이썬
```python
import numpy as np
# 공부한 시간과 점수를 각각 x, y라는 이름의 넘파이 배열로 만듭니다.
x = np.array([2, 4, 6, 8])
y = np.array([81, 93, 91, 97])
#x의 평균값을 구합니다.
mx = np.mean(x)
#y의 평균값을 구합니다.
my = np.mean(y)
# 출력으로 확인합니다.
print("x의 평균값:", mx)
print("y의 평균값:", my)
# 기울기 공식의 분모 부분입니다.
divisor = sum([(i - mx)**2 for i in x])
# 기울기 공식의 분자 부분입니다.
def top(x, mx, y, my):
    d = 0
    for i in range(len(x)):
        d += (x[i] - mx) * (y[i] - my)
    return d
dividend = top(x, mx, y, my)
# 출력으로 확인합니다.
print("분모:", divisor)
print("분자:", dividend)
# 기울기 a를 구하는 공식입니다.
a = dividend / divisor
# y절편 b 를 구하는 공식입니다.
b = my - (mx*a)
# 출력으로 확인합니다.
print("기울기 a =", a)
print("y절편 b =", b)
```
### 평균제곱 오차 파이썬
```python
#가상의 기울기 a와 y 절편 b를 정합니다.
fake_a=3
fake_b=7
#공부 시간 x와 성적 y의 넘파이 배열을 만듭니다.
x = np.array([2, 4, 6, 8])
y = np.array([81, 93, 91, 97])
# y=ax + b에 가상의 a,b 값을 대입한 결과를 출력하는 함수입니다.
def predict(x):
    return fake_a * x + fake_b
# 예측 값이 들어갈 빈 리스트를 만듭니다.
predict_result = []
# 모든 x값을 한 번씩 대입하여 predict_result 리스트를 완성합니다.
for i in range(len(x)):
    predict_result.append(predict(x[i]))
    print("공부시간=%.f, 실제점수=%.f, 예측점수=%.f" % (x[i], y[i], predict(x[i])))
# 평균 제곱 오차 함수를 각 y값에 대입하여 최종 값을 구하는 함수입니다.
n=len(x)  
def mse(y, y_pred):
    return (1/n) * sum((y - y_pred)**2)
# 평균 제곱 오차 값을 출력합니다.
print("평균 제곱 오차: " + str(mse(y,predict_result)))
```

## 4장
기울기를 너무 크게 잡거나 작게 잡으면 오차가 커짐 적절한 기울기를 찾으면 오차가 최소화된다.
미분 기울기를 이용하는 경사하강법
경사하강법 : a의 미분을 구하고 반대방향으로 이동후 또 미분을 구하고 기울기가 0이 될때까지 반복
경사하강법 : 오차의 변화에 따라 이차 함수 그래프를 만들고 적절한 학습률을 설정해 미분 값이 0인ㅇ 지점을 구하는 것
학습률은 그때그때 다름..
![image](https://github.com/gryrryfh/AI-Class/assets/50912987/994d48ed-c58d-4065-a3fa-8efe6209de5f)
```python
!pip install matplotlib
import numpy as np
import matplotlib.pyplot as plt
#공부 시간 X와 성적 Y의 넘파이 배열을 만듭니다.
x = np.array([2, 4, 6, 8])
y = np.array([81, 93, 91, 97])
# 기울기 a와 절편 b의 값을 초기화합니다.
a = 0
b = 0
#학습률을 정합니다.
lr = 0.03
#몇 번 반복될지를 설정합니다. 
epochs = 2001
# x 값이 총 몇 개인지 셉니다.
n=len(x)
#경사 하강법을 시작합니다.
for i in range(epochs):                  # epoch 수 만큼 반복
    y_pred = a * x + b                   # 예측 값을 구하는 식입니다. 
    error = y - y_pred                   # 실제 값과 비교한 오차를 error로 놓습니다.
    a_diff = (2/n) * sum(-x * (error))   # 오차 함수를 a로 편미분한 값입니다. 
    b_diff = (2/n) * sum(-(error))       # 오차 함수를 b로 편미분한 값입니다. 
    a = a - lr * a_diff     # 학습률을 곱해 기존의 a 값을 업데이트합니다.
    b = b - lr * b_diff     # 학습률을 곱해 기존의 b 값을 업데이트합니다.
    if i % 100 == 0:        # 100번 반복될 때마다 현재의 a 값, b 값을 출력합니다.
        print("epoch=%.f, 기울기=%.04f, 절편=%.04f" % (i, a, b))
#앞서 구한 최종 a값을 기울기, b값을 y절편에 대입하여 그래프를 그립니다.
y_pred = a * x + b      
#그래프 출력
plt.scatter(x, y)
plt.plot(x, y_pred,'r')
plt.show()
```

## 5장
새로운 용어들은 암기를 할 수밖에 없음

이해를 하고 암기를 하면 제일 좋지만 이해가 수밖에 없음

데이터를 불러올 때 여러 데이터 유형이 있을 때는 CSV나 아니면 리스트 파이썬의 리스를 사용함
딥러닝을 공부할 때 스터디 그룹을 만들어서 학습을 시키면 조작이 되면 10장부터 15장까지 나옴

16장부터는 활용하기를 해야 함

예측을 위한 로이스 테이를 사용함

딥러닝 인공지능에서는 기울기라고 표현 안 하고 웨이트라고 표현함

평균 제곱 오차는 손실 함수라는 표현을 사용함
경사하각법은 머릿속에 그려진 2차 함수가 있을 때 기울기 값을 계속 해가지고 0이 될 때까지 구하는 것임

최적화는 이미의 값을 하나 주고 옆에서부터 계속 값을 찾아내는 과정임

경사하각법은 옵티마이드로 찾아가는 방법임
환경 설정이 끝났으면 학습을 시켜야 함

공부를 한 번만 하지 말고 미친 놈 공부하라고 정의를 하고 있음

x와 y는 주어진 값임

x와 y를 그래프로 그리고 싶으면 파이썬 프합이라는 게 있음

피처 값으로 테스트를 위해서 데이터 파일로부터 보는 게 아니고 리스크를 줌
학습을 시켰으니까 예측을 해봐야 함

학습이 끝나고 새로운 데이터를 주면 학습한 모델에서 어떤 값을 판결해 줄 것인지 예측을 해봐야 함

예측을 하는 게 신용 행위임
기본 관리가 안 되면 인터페이스 자료로 쓰고 싶어 하는데 내가 찾는 자료가 맞는지 안 맞는지를 말할 거임

한계가 있으면 그 과목은 더 안 하게 됨

머릿속에 개념을 가지고 접근하라고 함
1차 함수는 y 값이 무수히 많은 값이 있을 수 있음

y 값이 x 축이고 x 축이 y 축이니까 y 값이 무수히 많은 값이 있을 수 있음
일상생활에도 레스모 형태의 문제들이 많음

우리 머릿속에 1이냐 0이냐의 문제는 로이스템이다 이렇게 머릿속에 딱 가지고 있으면 다 해결됨

합격이면 1인이랑 합격인 것처럼 에스자 그래프가 필요함

선형 회기를 먼저 이야기한 이유가 설명 로이스트 회기도 따지고 보면 그래프의 문제다 이 이야기를 하고 싶은 거임
오차 값이 0에 가까울수록 좋음

오차 값이 적을 때는 파란색 그래프를 사용하는 게 좋음

두 개의 그래프를 짬뽕해서 사용할 수 있음
리니어 문제가 아니고 그래프를 만들어야 됨

0과 1 사이에 그래프가 그려지니까 가능함

우리가 지금까지 인간 분해가 몇 개의 인물로 이루어져 있는지 찾아보라고 함
뉴럴 네트워크를 인공지능으로 만들 때 기본이 되는 것은 뉴럴 네트워크임

뉴럴 네트워크를 어디에 이용해 볼 것인지 설명함

앞에 있는 내용이 이해가 안 되면 질문하라고 함
