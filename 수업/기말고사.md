## 퍼셉트론
1. 퍼셉트론 : 로젠 블렛이 1957년에 고안한 알고리즘(인공신경망의 한 종류) (입력-가중합-활성화함수-출력)
2. 신경망의 기본 구조 : 여러 층의 퍼셉트론을 서로 연결시키고 복잡하게 조합하여 주어진 입력 값에 대한 판단을 하게 하는 것
3. 퍼셉트론의 한계 : xor 해결 불가, 다층 퍼셉트론(multilayer perceptron), 오차 역전파(back propagation)로 해결
4. 다중 퍼셉트론 : 퍼셉트론 두 개를 각각 처리하는 은닉층(hidden layer)을 입력층과 출력층 사이에 만들어 하나의 퍼셉트론으로 해결되지 않는 문제 해결, nand, or한 결과를 and함 이걸 알아내는데 20년 걸림(인공지능의 겨울)

## 다중 퍼셉트론, 오차역전파
1. 다중퍼셉트론을 쓰더라도 은닉층에 포함된 가중치를 업데이트할 방법이 없음.
2. 오차역전파 : 오차를 점점 거슬러 올라가면서 다시 전파하는 것(다층 퍼셉트론 최적화 과정)
3. 델타식을 이용해 깊은 신경망의 계산이 가능
4. 활성화함수:  가중치 업데이트가 처음 층까지 전달되지 않는 현상이 생기는 문제가 발견
5. relu함수로 해결(x가 0보다 작을 때 모든 값을 0으로 처리하고,0보다 큰 값은 x를 그대로 사용하는 방법)
6. 그외 하이퍼볼릭 탄젠트(hyperbolic tangent) 함수, 소프트플러스(softplus) 함수
7. 고급 경사하강법 : 1. 확률적 경사하강법 : 속도가 확연히 빠르면서도 최적 해에 근사한 값을 찾아낸다, ->방향조절   2. 모멘텀(탄성, 가속도) : 경사하강법에 탄성을 더하는 것 ->   3. adam(정확도, 속도 향상)
8. 고급 경사하강법은 optimizer로 간단히 실행 가능, sigmoid, relu도 activation으로 간단히 실행 가능

## 딥러닝모델 설계
1. sequential()함수 : 케라스
2. model.add로 새로운 층 만듦
3. 데이터에서 값을 16개 받아 은닉층의 노드 30개로 보낸다 **(model.add(Dense(30, input_dim=16, activation='relu')))**
4. 활성화 함수로 시그모이드(sigmoid) 함수를 사용 **(model.add(Dense(1, activation='sigmoid')))**
5. adam 사용, 평균 제곱 계열 중 하나 이항 분류로 컴파일 **model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])**
6. 5번 실행, 샘플을 16번씩 끊어 집어넣음 **(history=model.fit(X, y, epochs=5, batch_size=16))**

## 인디언당뇨병

## 다중분류
1. 다중분류 : 여러개 중에서 하나를 고르는 것(이항분류 못씀)
2. 원핫인코딩 :  여러 개의 값으로 된 문자열을 0과 1로만 이루어진 형태로 만들어 주는 과정(get_dummies() 함수)
3. 소프트맥스 : 첫째 출력층의 노드 수가 3으로 바뀜,  활성화 함수가 softmax로 바뀜,  마지막으로 컴파일 부분에서 손실 함수 부분이 categorical_crossentropy로 바뀜(각 샘플당 예측 확률의 총합이 1인 형태로 바꾸어 주게 됨)
4. 다항분류 : model.compile(loss='**categorical_crossentropy**', optimizer='adam', metrics=['accuracy'])

## 모델 성능 검증
1. 과적합 : 모델이 학습 데이터셋 안에서는 일정 수준 이상의 예측, 정확도를 보이지만, 새로운 데이터에 적용하면 잘 맞지 않는 것, 층이 너무 많거나 변수가 복잡해서 발생하기도 하고 테스트셋과 학습셋이 중복될 때 생기기도 함
2. 과적합 방지 : 테스트셋, 학습셋구분(학습이 계속되면 학습셋에서의 에러는 계속해서 작아지지만, 테스트셋에서는 과적합이 발생)
3. 식이 복잡해지고 학습량이 늘어날수록 학습 데이터를 통한 예측률은 계속해서 올라가지만, 적절하게 조절하지 않을 경우 테스트셋을 이용한 예측률은 오히려 떨어지는 것을 확인할 수 있음
4. 모델 성능의 향상을 위한 방법 : 데이터 보강, 알고리즘 최적화
5. 딥러닝이 아닌 랜덤 포레스트, XGBoost, SVM 등 다른 알고리즘이 더 좋은 성과를 보일 때도 있음, 일반적인 머신 러닝과 딥러닝을 합해서 더 좋은 결과를 만드는 것도 가능, 많은 경험을 통해 최적의 성능을 보이는 모델을 만드는 것이 중요
6. model.save() 함수를 이용해 모델 이름을 적어 저장, 다시불러오기는 케라스 API의 load_model 함수를 사용
7. k겹 교차검증 : k겹 교차 검증이란 데이터셋을 여러 개로 나누어 하나씩 테스트셋으로 사용하고 나머지를 모두 합해서 학습셋으로 사용하는 방법 데이터를 효율적으로 사용함(사이킷런 라이브러리의 KFold() 함수, 샘플이 어느 한쪽에 치우치지 않도록 shuffle 옵션을 True로 설정)
8. verbose=0 옵션을 주어 학습 과정의 출력을 생략

## 모델 성능 향상
1. 데이터셋+학습셋  +검증셋(최적의 학습 파라미터를 찾기 위해 학습 과정에서 사용하는 것)(validation_split이라는 옵션을 주면 만들어짐)
2. 모델 업데이트 : 학습 중인 모델을 저장하는 함수는 케라스 API의 ModelCheckpoint(), verbose는 1(True) 모델 모니터링
3. loss : 오차, accuracy : 정확도, val_loss : 검증셋 오차, val_accuracy : 검증셋 정확도
4. 학습 자동중단 : 텐서플로에 포함된 케라스 API는 EarlyStopping() 함수
5. monitor='val_loss', patience=20이라고 지정하면 검증셋의 오차가 20번 이상 낮아지지 않을 경우 학습을 종료

## 실제 모델 생성
1. 데이터파악, 결측치, 변수처리 : 결측치가 있는지 알아보는 함수는 isnull()
2. get_dummies() 함수를 이용해 카테고리형 변수를 0과 1로 이루어진 변수
3.  결측치를 채워 주는 함수는 판다스의 fillna()
4.  dropna() : 결측치 속성 제거
5.  속성별 관련도 추출
6.  입력될 속성의 개수를 X_train.shape[1]로 지정해 자동으로 세도록
7.  선형 회귀이므로 평균 제곱 오차(mean_squared_error)를 적음

## 그외 1장부터 6?장까지
1. 평균제곱오차 : model.compile(loss=‘mean_squared_error', optimizer='adam', metrics=['accuracy'])
2. 경사하강법 : optimizer=adam
3. 로지스틱 회귀 : 직선이 아닌 참(1)과 거짓(0) 사이를 구분하는 S자 형태의 선을 그어주는 과정
4. 활성화함수 : 0과1을 판단하여 다음으로 보내는 함수(model.add(Dense(30, input_dim=17, activation='relu')) model.add(Dense(1, activation='sigmoid')))
5. 

